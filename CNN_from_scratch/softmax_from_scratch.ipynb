{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lfPhFeQAQYBK"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "import numpy\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(g):\n",
        "    # apply simple softmax\n",
        "    y = cp.exp(g)\n",
        "    return y / cp.sum(y)\n",
        "\n",
        "def backward_propagation(g, true_labels):\n",
        "\n",
        "    # variant with creating matrix\n",
        "    matrix = cp.zeros((len(true_labels), len(true_labels)))\n",
        "    for i in range(len(true_labels)):\n",
        "        for j in range(len(true_labels)):\n",
        "            e1 = cp.exp(g[i])\n",
        "            e2 = cp.exp(g[j])\n",
        "            s1 = 1 / cp.sum(cp.exp(g))\n",
        "            s2 = -1 * (1 / cp.sum(cp.exp(g))**2)\n",
        "            if i == j:\n",
        "                matrix[i][j] = e1*s2*e2 + e1*s1\n",
        "            else:\n",
        "                matrix[i][j] = e1*s2*e2 + s1\n",
        "    return cp.dot(true_labels, matrix)\n",
        "\n",
        "    #   variant with computing gradients manually\n",
        "    # e = cp.exp(g)\n",
        "    # s = cp.sum(e)\n",
        "    # s = 1 / s\n",
        "    #   back to copy gate (2)\n",
        "    # g0 = true_labels * e\n",
        "    #   back to cope gate (1)\n",
        "    # g1 = true_labels * s\n",
        "    #   back to 1 / input gate\n",
        "    # g2 = cp.sum(g0)\n",
        "    #   back to add gate\n",
        "    # g3 = g2*(-1 * s**2)\n",
        "    #   back to exp gate\n",
        "    # g4 = g3 + g1\n",
        "    #   return the result\n",
        "    # return g4*e\n",
        "\n",
        "\n",
        "def forward_propagation_normalization(g):\n",
        "    # apply simple softmax and normalize (subtract maximum)\n",
        "    y = cp.exp(g - cp.max(g))\n",
        "    return y / cp.sum(y)\n",
        "\n",
        "def backward_propagation_normalization(g, true_labels):\n",
        "    # variant with creating matrix (the same, because the normalization does not affect the gradient computation)\n",
        "    matrix = cp.zeros((len(true_labels), len(true_labels)))\n",
        "    for i in range(len(true_labels)):\n",
        "        for j in range(len(true_labels)):\n",
        "            e1 = cp.exp(g[i])\n",
        "            e2 = cp.exp(g[j])\n",
        "            s1 = 1 / cp.sum(cp.exp(g))\n",
        "            s2 = -1 * (1 / cp.sum(cp.exp(g))**2)\n",
        "            if i == j:\n",
        "                matrix[i][j] = e1*s2*e2 + e1*s1\n",
        "            else:\n",
        "                matrix[i][j] = e1*s2*e2 + s1\n",
        "    return cp.dot(true_labels, matrix)\n",
        "\n",
        "    #   variant with computing gradients manually\n",
        "    # e = cp.exp(g)\n",
        "    # s = cp.sum(e)\n",
        "    # s = 1 / s\n",
        "    #   back to copy gate (2)\n",
        "    # g0 = true_labels * e\n",
        "    #   back to cope gate (1)\n",
        "    # g1 = true_labels * s\n",
        "    #   back to 1 / input gate\n",
        "    # g2 = cp.sum(g0)\n",
        "    #   back to add gate\n",
        "    # g3 = g2*(-1 * s**2)\n",
        "    #   back to exp gate\n",
        "    # g4 = g3 + g1\n",
        "    #   the normalization was applied by subtracting a constant (subtract maximum), thus the gradient will not change at this step\n",
        "    #   return the result\n",
        "    # return g4*e"
      ],
      "metadata": {
        "id": "ZwD7BJTeWzAQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the output with torch\n",
        "inputs = cp.array([[1, -1, 0, 0], [0, 99, 0, 0], [0, 1, 21, 4]])\n",
        "for input in inputs:\n",
        "    print()\n",
        "    print(f\"Forward pass: {forward_propagation(input)}\")\n",
        "    print(f\"Backward grad: {backward_propagation(input, cp.array([1, 0, 0, 0]))}\")\n",
        "    print(f\"Forward pass (normalization): {forward_propagation_normalization(input)}\")\n",
        "    print(f\"Backward grad (normalization): {backward_propagation_normalization(input, cp.array([1, 0, 0, 0]))}\")\n",
        "\n",
        "    input_tensor = torch.Tensor(input)\n",
        "    input_tensor.requires_grad = True\n",
        "    softmax = torch.nn.Softmax()\n",
        "    output_tensor = softmax(input_tensor)\n",
        "    print(f\"Forward pass (torch): {output_tensor.data.numpy()}\")\n",
        "    output_tensor.backward(torch.Tensor([1, 0, 0, 0]))\n",
        "    print(f\"Backward grad (torch): {input_tensor.grad.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMo6CdFDLIEM",
        "outputId": "07102d5a-c86a-4036-fffc-8865bffe1e22"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forward pass: [0.53444665 0.07232949 0.19661193 0.19661193]\n",
            "Backward grad: [0.24881343 0.15795568 0.09153335 0.09153335]\n",
            "Forward pass (normalization): [0.53444665 0.07232949 0.19661193 0.19661193]\n",
            "Backward grad (normalization): [0.24881343 0.15795568 0.09153335 0.09153335]\n",
            "Forward pass (torch): [0.53444666 0.07232949 0.19661194 0.19661194]\n",
            "Backward grad (torch): [ 0.24881342 -0.03865625 -0.10507859 -0.10507859]\n",
            "\n",
            "Forward pass: [1.01122149e-43 1.00000000e+00 1.01122149e-43 1.01122149e-43]\n",
            "Backward grad: [1.01122149e-43 0.00000000e+00 1.01122149e-43 1.01122149e-43]\n",
            "Forward pass (normalization): [1.01122149e-43 1.00000000e+00 1.01122149e-43 1.01122149e-43]\n",
            "Backward grad (normalization): [1.01122149e-43 0.00000000e+00 1.01122149e-43 1.01122149e-43]\n",
            "Forward pass (torch): [1.01e-43 1.00e+00 1.01e-43 1.01e-43]\n",
            "Backward grad (torch): [ 1.01e-43 -1.01e-43 -0.00e+00 -0.00e+00]\n",
            "\n",
            "Forward pass: [7.58256009e-10 2.06115353e-09 9.99999956e-01 4.13993754e-08]\n",
            "Backward grad: [7.58256009e-10 7.58256008e-10 3.35291594e-17 7.58255978e-10]\n",
            "Forward pass (normalization): [7.58256009e-10 2.06115353e-09 9.99999956e-01 4.13993754e-08]\n",
            "Backward grad (normalization): [7.58256009e-10 7.58256008e-10 3.35291594e-17 7.58255978e-10]\n",
            "Forward pass (torch): [7.5825601e-10 2.0611537e-09 1.0000000e+00 4.1399375e-08]\n",
            "Backward grad (torch): [ 7.5825601e-10 -1.5628822e-18 -7.5825601e-10 -3.1391326e-17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-a2287c76e059>:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output_tensor = softmax(input_tensor)\n"
          ]
        }
      ]
    }
  ]
}