{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lfPhFeQAQYBK"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "import numpy\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation_sm(g):\n",
        "    # apply simple softmax\n",
        "    y = cp.exp(g)\n",
        "    return y / cp.sum(y)\n",
        "\n",
        "def backward_propagation_sm(g, upstream_gradient):\n",
        "\n",
        "    matrix = cp.zeros((len(upstream_gradient), len(upstream_gradient)))\n",
        "    for i in range(len(upstream_gradient)):\n",
        "        for j in range(len(upstream_gradient)):\n",
        "            e1 = cp.exp(g[i])\n",
        "            e2 = cp.exp(g[j])\n",
        "            s1 = 1 / cp.sum(cp.exp(g))\n",
        "            s2 = -1 * (1 / cp.sum(cp.exp(g))**2)\n",
        "            if i == j:\n",
        "                matrix[i][j] = e1*s2*e2 + e1*s1\n",
        "            else:\n",
        "                matrix[i][j] = e1*s2*e2\n",
        "    return  upstream_gradient @ matrix\n",
        "\n",
        "\n",
        "def forward_propagation_sm_normalization(g):\n",
        "    # apply simple softmax and normalize (subtract maximum)\n",
        "    y = cp.exp(g - cp.max(g))\n",
        "    return y / cp.sum(y)\n",
        "\n",
        "def backward_propagation_sm_normalization(g, upstream_gradient):\n",
        "    # the same, because the normalization does not affect the gradient computation\n",
        "    matrix = cp.zeros((len(upstream_gradient), len(upstream_gradient)))\n",
        "    for i in range(len(upstream_gradient)):\n",
        "        for j in range(len(upstream_gradient)):\n",
        "            e1 = cp.exp(g[i])\n",
        "            e2 = cp.exp(g[j])\n",
        "            s1 = 1 / cp.sum(cp.exp(g))\n",
        "            s2 = -1 * (1 / cp.sum(cp.exp(g))**2)\n",
        "            if i == j:\n",
        "                matrix[i][j] = e1*s2*e2 + e1*s1\n",
        "            else:\n",
        "                matrix[i][j] = e1*s2*e2\n",
        "    return cp.dot(upstream_gradient, matrix)"
      ],
      "metadata": {
        "id": "ZwD7BJTeWzAQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# task 1\n",
        "\n",
        "input = cp.array([0, -1, 21, 4])\n",
        "dL_dy = cp.array([1, 3, 4, 2])\n",
        "\n",
        "# forward pass\n",
        "y_value = forward_propagation_sm(input)\n",
        "\n",
        "# backpropogation\n",
        "dL_dx = backward_propagation_sm(input, dL_dy)\n",
        "\n",
        "print(f\"Forward pass: {y_value}\")\n",
        "print(f\"Backpropagation: {dL_dx}\")\n",
        "\n",
        "print()\n",
        "\n",
        "input_tensor = torch.Tensor(input)\n",
        "input_tensor.requires_grad = True\n",
        "softmax = torch.nn.Softmax()\n",
        "output_tensor = softmax(input_tensor)\n",
        "print(f\"Forward pass (torch): {output_tensor.data.numpy()}\")\n",
        "output_tensor.backward(torch.Tensor(dL_dy))\n",
        "print(f\"Backpropagation (torch): {input_tensor.grad.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF3f56NpYvqr",
        "outputId": "621cadfe-2fde-4675-856b-b1299df23884"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward pass: [7.58256011e-10 2.78946797e-10 9.99999958e-01 4.13993754e-08]\n",
            "Backpropagation: [-2.27476797e-09 -2.78946774e-10  8.53524618e-08 -8.27987473e-08]\n",
            "\n",
            "Forward pass (torch): [7.5825601e-10 2.7894681e-10 1.0000000e+00 4.1399375e-08]\n",
            "Backpropagation (torch): [-2.274768e-09 -2.789468e-10  0.000000e+00 -8.279875e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-fb9790efc1ed>:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output_tensor = softmax(input_tensor)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation_rl(g):\n",
        "    # note that d(ReLU)/dx (0) = 0 was chosen\n",
        "    return cp.vectorize(lambda x: max(0, x))(g)\n",
        "\n",
        "def backward_propagation_rl(g, upstream_gradient):\n",
        "    matrix = cp.zeros((len(upstream_gradient), len(upstream_gradient)))\n",
        "    for i in range(len(upstream_gradient)):\n",
        "        for j in range(len(upstream_gradient)):\n",
        "            if i == j:\n",
        "                matrix[i][j] = 1 if g[i] > 0 else 0\n",
        "            else:\n",
        "                matrix[i][j] = 0\n",
        "    return cp.dot(upstream_gradient, matrix)"
      ],
      "metadata": {
        "id": "fhSanNcfg_Dy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# task 2\n",
        "\n",
        "input = cp.array([0, -1, 21, 4])\n",
        "dL_dy = cp.array([1, 3, 4, 2])\n",
        "\n",
        "# forward pass\n",
        "y_value = forward_propagation_rl(input)\n",
        "\n",
        "# backpropogation\n",
        "dL_dx = backward_propagation_rl(input, dL_dy)\n",
        "\n",
        "print(f\"Forward pass: {y_value}\")\n",
        "print(f\"Backpropagation: {dL_dx}\")\n",
        "\n",
        "print()\n",
        "\n",
        "input_tensor = torch.Tensor(input)\n",
        "input_tensor.requires_grad = True\n",
        "relu = torch.nn.ReLU()\n",
        "output_tensor = relu(input_tensor)\n",
        "print(f\"Forward pass (torch): {output_tensor.data.numpy()}\")\n",
        "output_tensor.backward(torch.Tensor([1, 3, 4, 2]))\n",
        "print(f\"Backpropagation (torch): {input_tensor.grad.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yGTRPbgYyEK",
        "outputId": "a7c5b3a0-20da-45e4-dc95-711c1dc5eb03"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward pass: [ 0  0 21  4]\n",
            "Backpropagation: [0. 0. 4. 2.]\n",
            "\n",
            "Forward pass (torch): [ 0.  0. 21.  4.]\n",
            "Backpropagation (torch): [0. 0. 4. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_W(input_dim, output_dim):\n",
        "    W_dim = (output_dim[0], input_dim[0])\n",
        "    W = cp.random.rand(*W_dim)\n",
        "    return W\n",
        "\n",
        "def forward_pass(g, W):\n",
        "    return W @ g\n",
        "\n",
        "def backpropagation(W, upstream_gradient):\n",
        "    return W.T.dot(upstream_gradient)"
      ],
      "metadata": {
        "id": "4KGeQaygkBet"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# task 3\n",
        "\n",
        "input_dim =(2, 3)\n",
        "output_dim = (5, 3)\n",
        "W = create_W(input_dim, output_dim)\n",
        "\n",
        "input = cp.random.rand(*input_dim)\n",
        "dL_dy = cp.random.rand(*output_dim)\n",
        "\n",
        "# forward pass\n",
        "y_value = forward_pass(input, W)\n",
        "\n",
        "# backpropogation\n",
        "dL_dx = backpropagation(W, dL_dy)\n",
        "\n",
        "print(f\"Forward pass: {y_value}\")\n",
        "print(f\"Backpropagation: {dL_dx}\")\n",
        "\n",
        "print()\n",
        "\n",
        "input_tensor = torch.Tensor(input)\n",
        "input_tensor.requires_grad = True\n",
        "output_tensor = torch.Tensor(W) @ input_tensor\n",
        "print(f\"Forward pass (torch): {output_tensor.data.numpy()}\")\n",
        "output_tensor.backward(torch.Tensor(dL_dy))\n",
        "print(f\"Backpropagation (torch): {input_tensor.grad.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g2uPeEfYytP",
        "outputId": "540b963d-45d5-4d37-e4bc-c720263022b6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward pass: [[0.2515419  0.39103843 0.28332879]\n",
            " [0.55773177 0.75237831 0.53440061]\n",
            " [0.32120092 0.3022164  0.20051022]\n",
            " [0.67417465 0.66349992 0.44472393]\n",
            " [0.36416323 0.49937562 0.35557392]]\n",
            "Backpropagation: [[1.31684986 1.0139439  0.9456753 ]\n",
            " [1.42927258 1.55612058 1.51959441]]\n",
            "\n",
            "Forward pass (torch): [[0.2515419  0.39103842 0.28332877]\n",
            " [0.55773175 0.7523783  0.5344006 ]\n",
            " [0.32120094 0.3022164  0.20051023]\n",
            " [0.6741747  0.66349995 0.44472393]\n",
            " [0.36416325 0.49937564 0.35557392]]\n",
            "Backpropagation (torch): [[1.3168498 1.0139439 0.9456753]\n",
            " [1.4292725 1.5561205 1.5195944]]\n"
          ]
        }
      ]
    }
  ]
}