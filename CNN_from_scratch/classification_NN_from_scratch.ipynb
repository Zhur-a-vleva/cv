{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6l86BNpvxx2g"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##As previous functions was too simple to construct NN, let's construct classes for simple usage"
      ],
      "metadata": {
        "id": "q3lFTRCmLW6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent():\n",
        "\n",
        "\n",
        "    def __init__(self, lr = 1e-3, eps = 1e-4):\n",
        "        self.lr = lr\n",
        "        self.eps = eps\n",
        "        self.delta = 0\n",
        "\n",
        "\n",
        "    def optimize(self, target, gradients):\n",
        "        optimized = []\n",
        "        for t, grad in zip(target, gradients):\n",
        "            optimized.append(t - self.lr * grad)\n",
        "            self.delta += self.lr * np.linalg.norm(grad)\n",
        "        return optimized\n",
        "\n",
        "\n",
        "    def stop(self):\n",
        "        return not(self.delta > 1e-9 or self.delta < self.eps)"
      ],
      "metadata": {
        "id": "X3WVInDK6TGy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, inner_ndim):\n",
        "\n",
        "        self.n_input = (input_dim, ) if isinstance(input_dim, int) else tuple(input_dim)\n",
        "        self.input_dim = 1 if isinstance(input_dim, int) else len(self.n_input)\n",
        "        self.n_output = (output_dim, ) if isinstance(output_dim, int) else tuple(output_dim)\n",
        "        self.output_dim = 1 if isinstance(input_dim, int) else len(self.n_output)\n",
        "        self.inner_dim = inner_ndim\n",
        "        self.input = None\n",
        "        self.labels = None\n",
        "\n",
        "\n",
        "\n",
        "    def change_dims(self, x, dim):\n",
        "        return np.reshape(x, x.shape[-dim:]) if x.ndim > dim else (x if x.ndim == dim else np.expand_dims(x, tuple(range(dim - x.ndim))))"
      ],
      "metadata": {
        "id": "uYr8NWbVGieT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OvSy-KF3w2St"
      },
      "outputs": [],
      "source": [
        "class SoftmaxLoss(Node):\n",
        "\n",
        "\n",
        "    def __init__(self, n_input):\n",
        "        super().__init__(n_input, 1, 2)\n",
        "\n",
        "\n",
        "    def softmax_call(self, x):\n",
        "        self.sm_max_index = np.argmax(np.abs(x), axis=1).reshape(-1, 1)\n",
        "        self.softmax_x_norm = x / np.max(np.abs(x), axis=1).reshape(-1, 1)\n",
        "        exp = np.exp(self.softmax_x_norm)\n",
        "        return exp / exp.sum(axis=1).reshape(-1, 1)\n",
        "\n",
        "\n",
        "    def softmax_jacobian(self, x):\n",
        "        rows, classes = self.softmax_x_norm.shape\n",
        "        exp_x = np.exp(self.softmax_x_norm)\n",
        "        exp_sum = exp_x.sum(axis=1)\n",
        "\n",
        "        softmax_jacobian = np.zeros((rows, classes, classes))\n",
        "\n",
        "        for row in range(rows):\n",
        "            exp_x_row = exp_x[row]\n",
        "            exp_sum_row = exp_sum[row]\n",
        "            diag = np.diag([exp_xi / exp_sum_row - (exp_xi / exp_sum_row) ** 2 for exp_xi in exp_x_row])\n",
        "            triag = np.array([[-exp_x_row[i] * exp_x_row[j] / exp_sum_row**2 if i > j else 0 for i in range(classes)] for j in range(classes)])\n",
        "            softmax_jacobian[row] = diag + triag + triag.T\n",
        "\n",
        "        for row in range(x.shape[0]):\n",
        "            max_index = self.sm_max_index[row][0]\n",
        "            x_max = np.abs(x)[row, max_index]\n",
        "            dx_norm_dx = np.diag([1/x_max for _ in range(x.shape[1])])\n",
        "            dx_norm_dx[:, max_index] = np.array([-x_i / x_max**2 for x_i in x[row]])\n",
        "            dx_norm_dx[max_index] = np.zeros(x.shape[1])\n",
        "            softmax_jacobian[row] = softmax_jacobian[row] @ dx_norm_dx\n",
        "\n",
        "        return softmax_jacobian\n",
        "\n",
        "\n",
        "    def jacobian(self, x):\n",
        "        softmax_jac = self.softmax_jacobian(x)\n",
        "        jac = np.zeros_like(x)\n",
        "        for i in range(x.shape[0]):\n",
        "            jac[i] = - self.labels[i] * softmax_jac[i].diagonal() / self.loss_elementwise[i]\n",
        "        return jac\n",
        "\n",
        "\n",
        "    def forward(self, input, labels = None):\n",
        "        self.input = self.change_dims(input, self.inner_dim)\n",
        "        self.labels = self.change_dims(labels, self.inner_dim)\n",
        "        self.loss_elementwise = np.array([-np.log(self.labels[i].dot(self.softmax_call(self.input)[i])) for i in range(self.input.shape[0])])\n",
        "        return self.change_dims(np.sum(self.loss_elementwise) / self.input.shape[0], self.output_dim)\n",
        "\n",
        "\n",
        "    def backward(self):\n",
        "        backprop_pd = self.jacobian(self.input)\n",
        "        self.input = None\n",
        "        self.labels = None\n",
        "        return self.change_dims(backprop_pd, self.input_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(Node):\n",
        "\n",
        "\n",
        "    def __init__(self, n_input: int):\n",
        "        super().__init__(n_input, n_input, 2)\n",
        "\n",
        "\n",
        "    def jacobian(self, x):\n",
        "        return np.array([np.diag([1 if x_elem > 0 else 0 for x_elem in x_row]) for x_row in x])\n",
        "\n",
        "\n",
        "    def forward(self, input, labels = None):\n",
        "        self.input = self.change_dims(input, self.inner_dim)\n",
        "        return self.change_dims(np.maximum(self.input, 0), self.output_dim)\n",
        "\n",
        "\n",
        "    def backward(self, input_pd):\n",
        "        input_pd = self.change_dims(input_pd, self.inner_dim)\n",
        "        jacobian = self.jacobian(self.input)\n",
        "        backprop_pd = np.array([jacobian[i] @ input_pd[i] for i in range(jacobian.shape[0])])\n",
        "        return self.change_dims(backprop_pd, self.output_dim)\n",
        "\n",
        "\n",
        "    def optimize_weights(self, optimizer):\n",
        "        pass"
      ],
      "metadata": {
        "id": "la9hfK_FF6vw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(Node):\n",
        "\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, W = None):\n",
        "        super().__init__(input_dim, output_dim, inner_ndim=2)\n",
        "        self.W_dim = (self.n_input[0]+1, self.n_output[0]) if self.input_dim == 1 else (self.n_input[1]+1, self.n_output[1])\n",
        "        self.W = np.random.uniform(0.4, 0.6, self.W_dim) if W is None else W\n",
        "        self.input = None\n",
        "        self.W_pd = None\n",
        "\n",
        "\n",
        "    def jacobian(self, X):\n",
        "        if self.input_dim == 1:\n",
        "            jac_dim = (1, self.n_input[0], 1, self.n_output[0])\n",
        "        else:\n",
        "            jac_dim = (*self.n_input, *self.n_output)\n",
        "\n",
        "        jac = np.zeros(jac_dim, dtype=np.float64)\n",
        "\n",
        "        for i in range(jac_dim[0]):\n",
        "            for j in range(jac_dim[1]):\n",
        "                jac[i, j, i] = self.W[j+1]\n",
        "        return jac\n",
        "\n",
        "\n",
        "    def W_jacobian(self, X):\n",
        "        if self.output_dim == 1:\n",
        "            jac_dim = (*self.W_dim, 1, self.n_output[0])\n",
        "        else:\n",
        "            jac_dim = (*self.W_dim, *self.n_output)\n",
        "\n",
        "        jac = np.zeros(jac_dim, dtype=np.float64)\n",
        "        for i in range(jac_dim[0]):\n",
        "            for j in range(jac_dim[1]):\n",
        "                jac[i, j, :, i] = X[:, i]\n",
        "        return jac\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = self.change_dims(input, self.inner_dim)\n",
        "        self.input = np.concatenate([np.ones((self.input.shape[0], 1)), self.input], axis=1)\n",
        "        return self.change_dims(self.change_dims(self.input @ self.W, self.output_dim), self.output_dim)\n",
        "\n",
        "\n",
        "    def backward(self, input_pd):\n",
        "        input_pd = self.change_dims(input_pd, self.inner_dim)\n",
        "        input_pd = input_pd.astype(np.float64)\n",
        "        self.W_pd = self.input.T.dot(input_pd)\n",
        "        return self.change_dims(input_pd.dot(self.W[1:, :].T), self.output_dim)\n",
        "\n",
        "\n",
        "    def optimize_weights(self, gd):\n",
        "        self.W = gd.optimize([self.W], [self.W_pd])[0]"
      ],
      "metadata": {
        "id": "slJXRaLuLU3K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxNN:\n",
        "\n",
        "\n",
        "    def __init__(self, n_input, n_output, batch_size, lr):\n",
        "        self.layers = [Linear((batch_size, n_input), (batch_size, n_output))]\n",
        "        self.loss = SoftmaxLoss((batch_size, n_output))\n",
        "        self.gd = GradientDescent(lr)\n",
        "\n",
        "\n",
        "    def fit(self, X, y, n_epochs):\n",
        "        n = 0\n",
        "        while True:\n",
        "            loss = 0\n",
        "            for batch in range(X.shape[0]):\n",
        "                state = X[batch]\n",
        "                label = y[batch]\n",
        "\n",
        "                state = self.predict(state)\n",
        "                loss += self.loss.forward(state, label)\n",
        "\n",
        "                upstream = self.loss.backward()\n",
        "                for layer in self.layers[::-1]:\n",
        "                    upstream = layer.backward(upstream)\n",
        "                    layer.optimize_weights(self.gd)\n",
        "            n += 1\n",
        "            print(f\"Epoch {n}, Loss: {loss}\")\n",
        "\n",
        "            if n >= n_epochs or self.gd.stop():\n",
        "                break\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        state = x.copy()\n",
        "        for layer in self.layers:\n",
        "            state = layer.forward(state)\n",
        "        return state"
      ],
      "metadata": {
        "id": "c9TbVxkU3Bh5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax NN"
      ],
      "metadata": {
        "id": "fDHUEFztxk53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_vec_func(labels):\n",
        "    labels_matrix = np.zeros([len(labels), 10])\n",
        "    labels_matrix[np.arange(len(labels)), labels] = 1\n",
        "    return labels_matrix"
      ],
      "metadata": {
        "id": "mkQkCw3a74b3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "n_input, n_output, batch_size = 784, 10, 2000\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0] // batch_size, batch_size, -1)\n",
        "y_train_one_hot = label_vec_func(y_train).reshape((y_train.shape[0] // batch_size, batch_size, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvMVI1_lxN2o",
        "outputId": "fc280c3d-a038-4fe9-f42c-4155890ee935"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network = SoftmaxNN(n_input, n_output, batch_size, 0.5)\n",
        "network.fit(X_train, y_train_one_hot, n_epochs=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "243kRM3HxWf_",
        "outputId": "90c265ff-cf55-4470-bf71-7de7f17fb5d6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: [66.37385665]\n",
            "Epoch 2, Loss: [64.83701333]\n",
            "Epoch 3, Loss: [64.35943009]\n",
            "Epoch 4, Loss: [64.09839416]\n",
            "Epoch 5, Loss: [63.9277188]\n",
            "Epoch 6, Loss: [63.80497339]\n",
            "Epoch 7, Loss: [63.7112725]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(X_test, y_test, model):\n",
        "\n",
        "    correct_predictions = 0\n",
        "    total = 0\n",
        "\n",
        "    for input, label in zip(X_test, y_test):\n",
        "        predicts = model.predict(input)\n",
        "        correct_predictions += (np.argmax(predicts, axis=1) == label).sum()\n",
        "        total += len(label)\n",
        "\n",
        "    return correct_predictions / total"
      ],
      "metadata": {
        "id": "H3mYtwxAxYHY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_batches = X_test.reshape((X_test.shape[0] // batch_size, batch_size, -1))\n",
        "y_batches = y_test.reshape((y_test.shape[0] // batch_size, batch_size,))"
      ],
      "metadata": {
        "id": "EhtCIZW71Org"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {compute_accuracy(X_batches, y_batches, network)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7I23IuRxZjP",
        "outputId": "c9b23b6a-26ac-4f97-b1a2-c643b7ccd639"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 layer NN"
      ],
      "metadata": {
        "id": "IWr9raXixfSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLUSoftmaxNN:\n",
        "\n",
        "\n",
        "    def __init__(self, n_input, n_output, batch_size, lr):\n",
        "        self.layers = [Linear((batch_size, X_train.shape[2]), (batch_size, 64)), ReLU((batch_size, 64)), Linear((batch_size, 64), (batch_size, 10))]\n",
        "        self.loss = SoftmaxLoss((batch_size, n_output))\n",
        "        self.gd = GradientDescent(lr)\n",
        "\n",
        "\n",
        "    def fit(self, X, y, n_epochs):\n",
        "        n = 0\n",
        "        while True:\n",
        "            loss = 0\n",
        "            for batch in range(X.shape[0]):\n",
        "                state = X[batch]\n",
        "                label = y[batch]\n",
        "\n",
        "                state = self.predict(state)\n",
        "                loss += self.loss.forward(state, label)\n",
        "\n",
        "                upstream = self.loss.backward()\n",
        "                for layer in self.layers[::-1]:\n",
        "                    upstream = layer.backward(upstream)\n",
        "                    layer.optimize_weights(self.gd)\n",
        "            n += 1\n",
        "            print(f\"Epoch {n}, Loss: {loss}\")\n",
        "\n",
        "            if n >= n_epochs or self.gd.stop():\n",
        "                break\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        state = x.copy()\n",
        "        for layer in self.layers:\n",
        "            state = layer.forward(state)\n",
        "        return state"
      ],
      "metadata": {
        "id": "11doS7Pk4LpH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = ReLUSoftmaxNN(784, 10, batch_size, 0.1)\n",
        "network.fit(X_train, y_train_one_hot, n_epochs=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndb-yYcAxeaV",
        "outputId": "54a38d1a-90f1-4feb-8562-c7894e238919"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: [69.06052055]\n",
            "Epoch 2, Loss: [69.04319154]\n",
            "Epoch 3, Loss: [69.0405569]\n",
            "Epoch 4, Loss: [69.04039233]\n",
            "Epoch 5, Loss: [69.04106566]\n",
            "Epoch 6, Loss: [69.04208959]\n",
            "Epoch 7, Loss: [69.04327179]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {compute_accuracy(X_batches, y_batches, network)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G6dunxGxowG",
        "outputId": "a50d2fc0-ce0c-45d3-c342-4936ddee5d62"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.1135\n"
          ]
        }
      ]
    }
  ]
}