{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "qk3c_VGZwQ7q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent():\n",
        "\n",
        "\n",
        "    def __init__(self, lr = 1e-3, eps = 1e-4):\n",
        "        self.lr = lr\n",
        "        self.eps = eps\n",
        "        self.delta = 0\n",
        "\n",
        "\n",
        "    def optimize(self, target, gradients):\n",
        "        optimized = []\n",
        "        for t, grad in zip(target, gradients):\n",
        "            optimized.append(t - self.lr * grad)\n",
        "            self.delta += self.lr * np.linalg.norm(grad)\n",
        "        return optimized\n",
        "\n",
        "\n",
        "    def stop(self):\n",
        "        return not(self.delta > 1e-9 or self.delta < self.eps)"
      ],
      "metadata": {
        "id": "X3WVInDK6TGy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, inner_ndim):\n",
        "\n",
        "        self.n_input = (input_dim, ) if isinstance(input_dim, int) else tuple(input_dim)\n",
        "        self.input_dim = 1 if isinstance(input_dim, int) else len(self.n_input)\n",
        "        self.n_output = (output_dim, ) if isinstance(output_dim, int) else tuple(output_dim)\n",
        "        self.output_dim = 1 if isinstance(input_dim, int) else len(self.n_output)\n",
        "        self.inner_dim = inner_ndim\n",
        "        self.input = None\n",
        "        self.labels = None\n",
        "\n",
        "\n",
        "\n",
        "    def change_dims(self, x, dim):\n",
        "        return np.reshape(x, x.shape[-dim:]) if x.ndim > dim else (x if x.ndim == dim else np.expand_dims(x, tuple(range(dim - x.ndim))))"
      ],
      "metadata": {
        "id": "uYr8NWbVGieT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolution(Node):\n",
        "\n",
        "\n",
        "    def __init__(self, input_dim, conv_dim, W = None):\n",
        "        super().__init__(input_dim, (1, input_dim[1] - conv_dim[1] + 1, input_dim[1] - conv_dim[1] + 1), 3)\n",
        "        self.W = np.random.uniform(0.4, 0.6, conv_dim) if W is None else W\n",
        "        self.input_values = None\n",
        "        self.output_values = None\n",
        "        self.labels = None\n",
        "        self.W_pd = None\n",
        "\n",
        "\n",
        "    def convolve(self, T, W, add_padding = False):\n",
        "        T = np.expand_dims(T, axis=0) if T.ndim == 2 else T\n",
        "        W = np.expand_dims(W, axis=0) if W.ndim == 2 else W\n",
        "        output_shape = (1, (T.shape[1] + W.shape[1] - 1), (T.shape[2] + W.shape[2] - 1)) if add_padding else (1, (T.shape[1] - W.shape[1] + 1), (T.shape[2] - W.shape[2] + 1))\n",
        "        T = np.pad(T, pad_width=[(0, 0), (W.shape[1] - 1, W.shape[1] - 1), (W.shape[2] - 1, W.shape[2] - 1)]) if add_padding else T\n",
        "\n",
        "        convolution = np.zeros(output_shape)\n",
        "        for row in range(output_shape[1]):\n",
        "            for col in range(output_shape[2]):\n",
        "                convolution[0, row, col] = np.sum(T[:, row: row + W.shape[1], col: col + W.shape[2]] * W)\n",
        "        return convolution\n",
        "\n",
        "\n",
        "    def forward(self, input, labels = None):\n",
        "        self.input_values = self.change_dims(input, self.inner_dim)\n",
        "        self.output_values = self.convolve(self.input_values, self.W)\n",
        "        return self.change_dims(self.output_values, self.output_dim)\n",
        "\n",
        "\n",
        "    def backward(self, input_pd):\n",
        "        self.W_pd = np.concatenate([self.convolve(self.input_values[i], self.change_dims(input_pd, self.inner_dim)) for i in range(self.n_input[0])], axis=0)\n",
        "        return self.change_dims(np.concatenate([self.convolve(self.change_dims(input_pd, self.inner_dim), self.W[i, ::-1, ::-1], True) for i in range(self.n_input[0])], axis=0), self.output_dim)\n",
        "\n",
        "\n",
        "    def optimize_weights(self, gd):\n",
        "        self.W = gd.optimize([self.W], [self.W_pd])[0]"
      ],
      "metadata": {
        "id": "zwcj6uMuw6E0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "yzDr40zxwemG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = (3, 5, 5)\n",
        "conv_dim = (3, 2, 2)\n",
        "conv = Convolution(input, conv_dim)\n",
        "\n",
        "x_input = np.random.random(input)\n",
        "dL_dy = np.random.random(conv.n_output)\n",
        "output = conv.forward(x_input)\n",
        "dL_dx = conv.backward(dL_dy)\n",
        "dL_dw = conv.W_pd\n",
        "\n",
        "x = tf.constant(np.moveaxis(np.expand_dims(x_input, axis=0), 1, -1), dtype=tf.float32)\n",
        "weights = tf.constant(np.moveaxis(conv.W, 0, -1), dtype=tf.float32)\n",
        "conv_keras = layers.Conv2D(1, 2, input_shape=x.shape[1:], use_bias=False, kernel_initializer=tf.keras.initializers.Constant(weights))"
      ],
      "metadata": {
        "id": "RplEP951wWap"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    tape.watch(x)\n",
        "    conv_output = conv_keras(x)\n",
        "\n",
        "print(f\"Keras: {conv_output.numpy().transpose(0, 3, 1, 2).squeeze()}\")\n",
        "print()\n",
        "print(f\"Mine: {output[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FKIgpjDwku7",
        "outputId": "75e14b0f-9dbd-4867-9694-6694e02902d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras: [[2.2348945 3.4954233 4.0519085 3.838164 ]\n",
            " [2.5698576 3.3950186 3.454504  3.4474192]\n",
            " [3.3045144 3.2550535 3.350473  3.3015566]\n",
            " [2.8904407 2.8494146 3.6963587 3.9793224]]\n",
            "\n",
            "Mine: [[2.23489443 3.4954233  4.05190854 3.83816412]\n",
            " [2.56985745 3.39501872 3.45450437 3.44741934]\n",
            " [3.30451437 3.2550536  3.3504725  3.30155665]\n",
            " [2.89044066 2.84941454 3.69635859 3.97932248]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dL_dy_keras = tf.constant(np.expand_dims(dL_dy, axis=-1), dtype=tf.float32)\n",
        "dL_dx_keras = tape.gradient(conv_output, x, output_gradients=dL_dy_keras)\n",
        "dL_dw_keras = tape.gradient(conv_output, conv_keras.trainable_variables, output_gradients=dL_dy_keras)\n",
        "\n",
        "print(f\"Keras: {np.moveaxis(dL_dx_keras[0].numpy().squeeze(), -1, 0)}\")\n",
        "print()\n",
        "print(f\"Mine: {dL_dx}\")\n",
        "print()\n",
        "print()\n",
        "print(f\"Keras: {np.moveaxis(dL_dw_keras[0].numpy().squeeze(), -1, 0)}\")\n",
        "print()\n",
        "print(f\"Mine: {dL_dw}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ9KMocWwojL",
        "outputId": "7bc356a2-05dd-4bad-a956-d4479b5e954a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras: [[[0.37909093 0.77021277 0.6416171  0.6713644  0.41283754]\n",
            "  [0.73011065 1.3666209  0.9097757  0.994215   0.755394  ]\n",
            "  [0.6286058  1.0950552  0.7051432  0.66914517 0.45234478]\n",
            "  [0.6090616  0.98327136 0.9517509  1.01633    0.42039365]\n",
            "  [0.32755104 0.51317054 0.4733592  0.6506156  0.308841  ]]\n",
            "\n",
            " [[0.46209475 0.8300403  0.6655717  0.7461585  0.37989756]\n",
            "  [0.8924444  1.4468099  0.93077147 1.1316056  0.7097536 ]\n",
            "  [0.76867914 1.1437469  0.736063   0.7396288  0.4259026 ]\n",
            "  [0.74422693 1.0163168  1.0580063  1.0656393  0.39159   ]\n",
            "  [0.40154198 0.52121985 0.54028285 0.6895323  0.29355246]]\n",
            "\n",
            " [[0.446396   0.938094   0.78887504 0.81122065 0.52142346]\n",
            "  [0.78528917 1.5282522  1.0040628  1.0671905  0.87655234]\n",
            "  [0.6668273  1.2415341  0.8267333  0.75031567 0.5201904 ]\n",
            "  [0.6627463  1.1197139  1.0645967  1.1774385  0.505856  ]\n",
            "  [0.3172918  0.54953426 0.4779792  0.6827619  0.34051234]]]\n",
            "\n",
            "Mine: [[[0.37909093 0.77021273 0.64161714 0.67136443 0.41283753]\n",
            "  [0.73011066 1.36662101 0.9097757  0.99421501 0.755394  ]\n",
            "  [0.62860581 1.09505531 0.70514324 0.66914516 0.45234477]\n",
            "  [0.60906159 0.98327131 0.9517509  1.01632993 0.42039365]\n",
            "  [0.32755105 0.51317057 0.47335921 0.65061556 0.30884098]]\n",
            "\n",
            " [[0.46209473 0.83004028 0.66557166 0.74615844 0.37989756]\n",
            "  [0.89244433 1.44680977 0.93077142 1.13160562 0.70975353]\n",
            "  [0.76867914 1.14374688 0.73606303 0.73962879 0.42590259]\n",
            "  [0.74422687 1.01631665 1.0580063  1.06563922 0.39158997]\n",
            "  [0.40154195 0.52121981 0.54028283 0.68953226 0.29355243]]\n",
            "\n",
            " [[0.44639598 0.93809405 0.78887509 0.81122068 0.52142348]\n",
            "  [0.78528916 1.52825231 1.00406279 1.06719053 0.87655236]\n",
            "  [0.66682737 1.24153419 0.82673324 0.75031567 0.52019041]\n",
            "  [0.66274627 1.11971396 1.06459664 1.17743846 0.50585596]\n",
            "  [0.31729178 0.54953423 0.4779792  0.68276186 0.34051232]]]\n",
            "\n",
            "\n",
            "Keras: [[[4.599048  5.3451366]\n",
            "  [3.6946821 5.8814735]]\n",
            "\n",
            " [[5.5305076 5.6762753]\n",
            "  [4.777061  5.745157 ]]\n",
            "\n",
            " [[4.1530657 4.2641487]\n",
            "  [4.367316  4.3460684]]]\n",
            "\n",
            "Mine: [[[4.5990481  5.34513696]\n",
            "  [3.6946821  5.88147364]]\n",
            "\n",
            " [[5.5305072  5.67627471]\n",
            "  [4.77706093 5.74515692]]\n",
            "\n",
            " [[4.15306572 4.26414898]\n",
            "  [4.36731566 4.34606824]]]\n"
          ]
        }
      ]
    }
  ]
}